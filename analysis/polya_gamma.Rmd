---
title: "polya gamma augmentation"
author: "DongyueXie"
date: "2020-09-21"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Definition

X follows polya-gamma distribution with parameters  $b>0$ and $c\in R$ if \[X\overset{D}{=}\frac{1}{2\pi^2}\sum_{k=1}^\infty\frac{g_k}{(k-1/2)^2+c^2/(4\pi^2)},\]
where $g_k\sim Gamma(b,1)$.

Binomial likelihoods parametrized by log odds can be represented as mixtures of Gaussians with respect to a P´olya-Gamma distribution.

## Properties

$$\frac{\left(e^{\psi}\right)^{a}}{\left(1+e^{\psi}\right)^{b}}=2^{-b} e^{\kappa \psi} \int_{0}^{\infty} e^{-\omega \psi^{2} / 2} p(\omega) d \omega,$$
where $\kappa = a-b/2$, and $\omega\sim PG(b,0)$.

The density of a Polya-Gamma random variable can be expressed as an alternating-sign sum
of inverse-Gaussian densities 
$$f(x \mid b, c)=\left\{\cosh ^{b}(c / 2)\right\} \frac{2^{b-1}}{\Gamma(b)} \sum_{n=0}^{\infty}(-1)^{n} \frac{\Gamma(n+b)}{\Gamma(n+1)} \frac{(2 n+b)}{\sqrt{2 \pi x^{3}}} e^{-\frac{(2 n+b)^{2}}{8 x}-\frac{c^{2}}{2} x}$$


All finite moments of a Polya-Gamma random variable are
available in closed form. In particular, the expectation may be calculated directly. This
allows the Polya-Gamma scheme to be used in EM algorithms. If $\omega\sim PG(b,c)$, then $E(\omega) = \frac{b}{2c}tanh(c/2) = \frac{b}{2c}(\frac{e^c-1}{1+e^c})$. The variance can be found [here](https://stats.stackexchange.com/questions/122957/what-is-the-variance-of-a-polya-gamma-distribution)

If $w_1\sim PG(b_1,c)$ and $w_2\sim PG(b_2,c)$ then $w_1+w_2\sim PG(b_1+b_2,c)$

## Augmentation


Let $y_i\sim Binomial(n_i,\frac{1}{1+e^{-\phi_i}})$, where $\phi_i$ are log odds of success. In logistic regression, $\phi_i = x_i^T\beta$.

THe likelihood contribution of observation $i$ is 

\[L_i(\phi_i) = \frac{(\exp\phi_i)^{y_i}}{(1+\exp(\phi_i))^{n_i}}.\]

In logistic regression, the likelihood is 

$$\begin{aligned}
L_{i}(\boldsymbol{\beta}) &=\frac{\left\{\exp \left(x_{i}^{T} \boldsymbol{\beta}\right)\right\}^{y_{i}}}{(1+\exp \left(x_{i}^{T} \boldsymbol{\beta}\right))^{n_i}} \\
& \propto \exp \left(\kappa_{i} x_{i}^{T} \boldsymbol{\beta}\right) \int_{0}^{\infty} \exp \left\{-\omega_{i}\left(x_{i}^{T} \boldsymbol{\beta}\right)^{2} / 2\right\} p\left(\omega_{i} \mid n_{i}, 0\right)
\end{aligned},$$

where $\kappa_i - y_i-n_i/2$.

The conditional posterior of $\beta$ is 
\[p(\beta|w,y)\propto p(\beta)\exp\{-\frac{1}{2}(z-X\beta)^T\Omega(z-X\beta)\} = p(\beta)\exp\{-\frac{1}{2}(\beta-X^{-1}z)^TX^T\Omega X(\beta-X^{-1}z)\},\]
where $z = (\kappa_1/w_1,...,\kappa_n/w_n)$ and $\Omega = diag(w_1,...,w_n)$.

## Posterior

If the prior of $\beta$ is Gaussian, then the canditional posterior of $\beta$ is also Gaussian. So the Gibbs sampler iteratively samples from $(\omega_i|\beta)\sim PG(n_i,x_i^T\beta), (\beta|y,\Omega)\sim N(m,V)$. 

## Simulation

### PG distribution

Histogram

```{r}
library(BayesLogit)
hist(rpg(1e4,0.1,0),breaks = 100)
hist(rpg(1e4,1,0),breaks = 100)
hist(rpg(1e4,10,0),breaks = 100)
hist(rpg(1e4,100,0),breaks = 100)

hist(rpg(1e4,1,0),breaks = 100)
hist(rpg(1e4,1,-1),breaks = 100)
hist(rpg(1e4,1,1),breaks = 100)
hist(rpg(1e4,1,100),breaks = 100)
```

Expectation

```{r}
pg_mean = function(b,c){b/(2*c)*tanh(c/2)}
cc = seq(-10,10,length=1000)
plot(cc,pg_mean(1,cc),type="l",ylim=c(0,0.25), main="E[PG(1,c)]",xlab='c',ylab='mean')
```


Variance 

\[var(\omega) = \frac{b}{4c^3}(sinh(c)-c)sech^2(c/2)\]

```{r}
pg_var = function(b,c){b/(4*c^3)*(sinh(c)-c)*(1/cosh(c/2))^2}
plot(cc,pg_var(1,cc),type="l",ylim=c(0,0.25), main="Var[PG(1,c)]",xlab='c',ylab='var')
```

### Bayesian logistics regression 

```{r}
# install_github('jwindle/BayesLogit',INSTALL_opts = '--no-lock')
set.seed(12345)
N = 300;
  P = 2;

  ##------------------------------------------------------------------------------
  ## Correlated predictors
  rho = 0.5
  Sig = matrix(rho, nrow=P, ncol=P); diag(Sig) = 1.0;
  U   = chol(Sig);
  X   = matrix(rnorm(N*P), nrow=N, ncol=P) %*% U;

  ##------------------------------------------------------------------------------
  ## Sparse predictors
  X   = matrix(rnorm(N*P, sd=1), nrow=N, ncol=P);
  vX  = as.numeric(X);
  low  = vX < quantile(vX, 0.5)
  high = vX > quantile(vX, 0.5);
  X[low]  = 0;
  X[!low] = 1;

  beta = rnorm(P, mean=0, sd=1);

  ## beta = c(1.0, 0.4);
  ## X = matrix(rnorm(N*P), nrow=N, ncol=P);

  psi = X %*% beta;
  p = exp(psi) / (1 + exp(psi));
  y = rbinom(N, 1, p);
  
  
psw.fit = logit.R(y,X)
beta
apply(psw.fit$beta,2,mean)
plot(psw.fit$beta[,1],type='l',ylab='draws')
plot(psw.fit$beta[,2],type='l',ylab='draws')
source('code/logistic.R')
prior <- list(mu = rep(0,P), Sigma = diag(1,P))
dr.fit = logit_CAVI(X,y,prior)
dr.fit$mu
plot(dr.fit$Convergence)
```

## Reference

Polson, N. G., Scott, J. G., & Windle, J. (2013). Bayesian inference for logistic models using Pólya–Gamma latent variables. Journal of the American statistical Association, 108(504), 1339-1349.
